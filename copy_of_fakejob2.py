# -*- coding: utf-8 -*-
"""Copy of FakeJob2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a1xISAIlx2UOmKtwsC-RdzBJa2-xR15C
"""

!pip install wordcloud

# Install required libraries
!pip install pandas numpy matplotlib seaborn scikit-learn nltk gradio wordcloud imbalanced-learn

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score
from scipy.sparse import hstack
import gradio as gr
import pickle
from google.colab import files
from wordcloud import WordCloud
from imblearn.over_sampling import SMOTE

# Upload and Load the file
uploaded = files.upload()
df = pd.read_csv("fake_job_postings.csv")
df.fillna('', inplace=True)

# Clean text function
def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Apply cleaning to important columns
for col in ['description', 'requirements', 'company_profile', 'title', 'benefits', 'salary_range']:
    df[col] = df[col].apply(clean_text)

# Combine important text fields
df['combined_text'] = (
    df['title'] + " " +
    df['description'] + " " +
    df['requirements'] + " " +
    df['company_profile'] + " " +
    df['benefits'] + " " +
    df['salary_range']
)

# Feature Engineering
df['has_high_salary'] = df['salary_range'].str.contains(r'\$|‚Çπ', case=False).astype(int)
df['has_easy_money_keywords'] = df['description'].str.contains(
    r'no experience|required|quick money|earn.*per week|work from home|easy money',
    case=False).astype(int)

# ‚úÖ ‚úÖ ‚úÖ VISUALIZATIONS ‚Äî Word Clouds
real_text = ' '.join(df[df['fraudulent'] == 0]['combined_text'])
fake_text = ' '.join(df[df['fraudulent'] == 1]['combined_text'])

plt.figure(figsize=(16, 8))

plt.subplot(1, 2, 1)
wordcloud_real = WordCloud(background_color='white', max_words=100).generate(real_text)
plt.imshow(wordcloud_real, interpolation='bilinear')
plt.axis('off')
plt.title('üü¢ Word Cloud - Real Job Descriptions')

plt.subplot(1, 2, 2)
wordcloud_fake = WordCloud(background_color='black', colormap='Reds', max_words=100).generate(fake_text)
plt.imshow(wordcloud_fake, interpolation='bilinear')
plt.axis('off')
plt.title('üî¥ Word Cloud - Fake Job Descriptions')

plt.tight_layout()
plt.show()

# TF-IDF vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_text = vectorizer.fit_transform(df['combined_text'])

# üîç TOP FEATURES (optional insight)
def show_top_tfidf_words(class_label, top_n=20):
    class_indices = df[df['fraudulent'] == class_label].index
    class_texts = vectorizer.transform(df.loc[class_indices, 'combined_text'])
    avg_tfidf = np.asarray(class_texts.mean(axis=0)).flatten()
    top_n_ids = avg_tfidf.argsort()[-top_n:][::-1]
    top_features = [vectorizer.get_feature_names_out()[i] for i in top_n_ids]
    scores = avg_tfidf[top_n_ids]

    plt.figure(figsize=(10, 5))
    sns.barplot(x=scores, y=top_features, palette='cool' if class_label == 0 else 'Reds_r')
    plt.title("Top TF-IDF Words - " + ("üü¢ Real" if class_label == 0 else "üî¥ Fake"))
    plt.xlabel("TF-IDF Score")
    plt.tight_layout()
    plt.show()

show_top_tfidf_words(0)  # Real
show_top_tfidf_words(1)  # Fake

# Stack TF-IDF with binary features
X_keywords = df[['has_high_salary', 'has_easy_money_keywords']]
X = hstack([X_text, X_keywords])
y = df['fraudulent']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Balance dataset using SMOTE
sm = SMOTE(random_state=42)
X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)

# Train Logistic Regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train_bal, y_train_bal)

# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Evaluation
print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("‚úÖ ROC-AUC Score:", roc_auc_score(y_test, y_prob))
print("‚úÖ Classification Report:\n", classification_report(y_test, y_pred))

# Save model & vectorizer
with open("fake_job_model.pkl", "wb") as file:
    pickle.dump(model, file)

with open("vectorizer.pkl", "wb") as file:
    pickle.dump(vectorizer, file)

# Reload for Gradio use
model = pickle.load(open("fake_job_model.pkl", "rb"))
vectorizer = pickle.load(open("vectorizer.pkl", "rb"))

# Clean text for prediction
def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Gradio prediction function
def predict_fake_job(job_description):
    clean_desc = clean_text(job_description)
    text_vec = vectorizer.transform([clean_desc])
    has_salary = int(bool(re.search(r'\$|‚Çπ', job_description, flags=re.I)))
    has_easy_money = int(bool(re.search(r'no experience|required|quick money|earn.*per week|work from home|easy money', job_description, flags=re.I)))
    input_final = hstack([text_vec, np.array([[has_salary, has_easy_money]])])
    prediction = model.predict(input_final)
    return "üü¢ Real Job Posting" if prediction[0] == 0 else "üî¥ Fake Job Posting"

# Gradio Interface
iface = gr.Interface(
    fn=predict_fake_job,
    inputs=gr.Textbox(lines=5, placeholder="Enter job description here..."),
    outputs="text",
    title="Fake Job Post Detector",
    description="Paste a job description to check if it's real or fake.",
)

iface.launch(share=True)