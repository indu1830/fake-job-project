# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BvJjLrnMhhWf88vo2YZ9VJ_fY09kSFKE
"""

# ✅ Step 1: Install required libraries
!pip install patool

# ✅ Step 2: Download UCF101 dataset (with SSL fix)
!wget --no-check-certificate -O UCF101.rar "https://www.crcv.ucf.edu/data/UCF101/UCF101.rar"

# ✅ Step 3: Install unrar if missing
!apt-get install unrar

# ✅ Step 4: Extract the dataset
import patoolib
import os

# Create output directory
DATASET_PATH = "/content/UCF101"
os.makedirs(DATASET_PATH, exist_ok=True)

# Extract the .rar file
patoolib.extract_archive("UCF101.rar", outdir=DATASET_PATH)

# ✅ Step 5: Check the extracted dataset
print("✅ Dataset extracted successfully!")
print("📂 Categories Available:")
print(os.listdir(DATASET_PATH))

import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Set dataset path
DATASET_PATH = "/content/UCF101/UCF-101"

# Select specific categories
selected_categories = ["ApplyEyeMakeup", "WalkingWithDog", "WritingOnBoard", "Surfing", "SkyDiving", "BabyCrawling",
                       "PlayingGuitar"]
activity_labels = selected_categories
num_classes = len(activity_labels)

print(f"🔹 Selected Activity Categories: {activity_labels}")

# Video parameters
IMG_SIZE = 112  # Reduce size to save memory
SEQUENCE_LENGTH = 30  # Number of frames per video

# Function to extract frames
def extract_frames(video_path, max_frames=30, img_size=112):
    cap = cv2.VideoCapture(video_path)
    frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.resize(frame, (img_size, img_size))  # Resize
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
        frame = frame / 255.0  # Normalize
        frames.append(frame)

    cap.release()

    # Ensure fixed number of frames
    while len(frames) < max_frames:
        frames.append(np.zeros((img_size, img_size)))  # Pad missing frames

    return np.array(frames[:max_frames]).reshape(max_frames, img_size, img_size, 1)

# Load videos and labels
def load_data(dataset_path, categories, num_samples=None):
    X, y = [], []

    for label in tqdm(categories, desc="Loading Categories"):
        folder_path = os.path.join(dataset_path, label)
        if not os.path.exists(folder_path):
            print(f"⚠ Warning: Category {label} not found!")
            continue  # Skip if not found

        videos = [os.path.join(folder_path, v) for v in os.listdir(folder_path) if v.endswith('.avi')]

        if num_samples:
            videos = videos[:num_samples]  # Limit per category

        if len(videos) == 0:
            print(f"⚠ No videos found for category: {label}")
            continue

        for video_path in tqdm(videos, desc=f"Processing {label}", leave=False):
            frames = extract_frames(video_path, img_size=112)
            if frames.shape[0] == 30:
                X.append(frames)
                y.append(activity_labels.index(label))

    return np.array(X), np.array(y)

# Load dataset with selected categories
X, y = load_data(DATASET_PATH, selected_categories, num_samples=50)  # Limit per category
y = to_categorical(y, num_classes)  # Convert labels to one-hot encoding

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"✅ Dataset Loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, TimeDistributed, LSTM, Dense, Dropout

# Build the LRCN Model
model = Sequential()

# CNN Feature Extractor
model.add(TimeDistributed(Conv2D(32, (3,3), activation='relu'), input_shape=(SEQUENCE_LENGTH, IMG_SIZE, IMG_SIZE, 1)))
model.add(TimeDistributed(MaxPooling2D((2,2))))
model.add(TimeDistributed(Conv2D(64, (3,3), activation='relu')))
model.add(TimeDistributed(MaxPooling2D((2,2))))
model.add(TimeDistributed(Flatten()))

# LSTM for temporal processing
model.add(LSTM(64, return_sequences=False))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation="softmax"))

# Compile the model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
print(model.summary())

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=4, validation_data=(X_test, y_test))  # Smaller batch size

# Load a test video
test_video_path = "/content/UCF101/UCF-101/PlayingGuitar/v_PlayingGuitar_g01_c01.avi"

# Extract frames from the test video
test_frames = extract_frames(test_video_path, img_size=112)

if test_frames.shape[0] == SEQUENCE_LENGTH:
    input_video = np.expand_dims(test_frames, axis=0)  # Reshape to match input format

    # Predict activity
    prediction = model.predict(input_video)
    predicted_label = activity_labels[np.argmax(prediction)]

    print(f"🔹 Recognized Activity: {predicted_label}")
else:
    print("❌ Error: Not enough frames in the test video")

cap = cv2.VideoCapture(test_video_path)
output_path = "output_video.avi"
fourcc = cv2.VideoWriter_fourcc(*'XVID')
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Display predicted activity
    cv2.putText(frame, f"Activity: {predicted_label}", (50, 50),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

    out.write(frame)

cap.release()
out.release()
cv2.waitKey(1)

print(f"✅ Processed video saved as: {output_path}")

test_video_path = "/content/UCF101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi"

# Extract frames from the test video
test_frames = extract_frames(test_video_path, img_size=112)

if test_frames.shape[0] == SEQUENCE_LENGTH:
    input_video = np.expand_dims(test_frames, axis=0)  # Reshape to match input format

    # Predict activity
    prediction = model.predict(input_video)
    predicted_label = activity_labels[np.argmax(prediction)]

    print(f"🔹 Recognized Activity: {predicted_label}")
else:
    print("❌ Error: Not enough frames in the test video")

cap = cv2.VideoCapture(test_video_path)
output_path = "output_video.avi"
fourcc = cv2.VideoWriter_fourcc(*'XVID')
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Display predicted activity
    cv2.putText(frame, f"Activity: {predicted_label}", (20, frame.shape[0] - 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)

    out.write(frame)


cap.release()
out.release()


print(f"✅ Processed video saved as: {output_path}")

from google.colab import files
files.download("output_video.avi")